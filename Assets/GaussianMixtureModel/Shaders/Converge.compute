#include "Utility.hlsl"
#include "Cholesky.hlsl"

#define GRID_SIZE 32
#define GROUP_SIZE 32

#pragma enable_d3d11_debug_symbols

#pragma kernel PrepareCholeskysAndLnDets
#pragma kernel UpdateRespsAndMeans
#pragma kernel ReduceWeightsAndMeans
#pragma kernel NormalizeMeansAndFracs
#pragma kernel UpdateCovariances
#pragma kernel ReduceCovariances
#pragma kernel NormalizeCovariances

RWStructuredBuffer<uint2> _SelectedColorBins;

Buffer<uint> _IndirectArgsBufferIn;
uint _IndirectArgsOffset;

StructuredBuffer<float> _WeightsIn;
RWStructuredBuffer<float> _WeightsOut;

StructuredBuffer<float> _RespsIn;
RWStructuredBuffer<float> _RespsOut;

StructuredBuffer<float> _FracsIn;
RWStructuredBuffer<float> _FracsOut;

StructuredBuffer<float3> _MeansIn;
RWStructuredBuffer<float3> _MeansOut;

StructuredBuffer<float2x3> _CovariancesIn;
RWStructuredBuffer<float2x3> _CovariancesOut;

StructuredBuffer<float> _LnDetsIn;
RWStructuredBuffer<float> _LnDetsOut;

StructuredBuffer<float3x3> _CholeskysIn;
RWStructuredBuffer<float3x3> _CholeskysOut;

uint _NumClusters;
int _ClusterIndex;
float _TotalSamples;

float3 GetVoxelCenter(uint index)
{
    uint3 index3d = To3DIndex(index, GRID_SIZE);
    return (index3d + (0.5).xxx) / (float)(GRID_SIZE).xxx;
}

[numthreads(GROUP_SIZE, 1, 1)]
void PrepareCholeskysAndLnDets(uint3 id : SV_DispatchThreadID)
{
    if (id.x > _NumClusters - 1)
    {
        return;
    }

    float3x3 covariance = ReadMatrixSymmetric3x3(_CovariancesIn, id.x);
    float3x3 cholesky = CholeskyCreate(covariance);
    _CholeskysOut[id.x] = cholesky;
    _LnDetsOut[id.x] = CholeskyLogDet(cholesky);
}

groupshared uint gs_IndirectArgsGroups;

// Each thread processes one voxel, over all clusters.
[numthreads(GROUP_SIZE, 1, 1)]
void UpdateRespsAndMeans(uint3 id : SV_DispatchThreadID)
{
    uint2 indexAndSize = _SelectedColorBins[id.x];
    float3 p = GetVoxelCenter(indexAndSize.x);
    float binSize = indexAndSize.y;
    
    uint eltsPerCluster = _IndirectArgsBufferIn[3];

    float maxResp = -9e99;

    // Evaluate responsibilities.
    [loop]
    for (uint k = 0; k != _NumClusters; ++k)
    {
        float3 u = p - _MeansIn[k];
        float3 v = 0;
        CholeskySolve(_CholeskysIn[k], u, v);
        float resp = -0.5 * (dot(v, v) + _LnDetsIn[k]) + log(_FracsIn[k]);
        maxResp = max(maxResp, resp);
        _RespsOut[eltsPerCluster * k + id.x] = resp;
    }

    // Log-sum-exp "trick".
    float sum = 0;
    for (k = 0; k != _NumClusters; ++k)
    {
        sum += exp(_RespsOut[eltsPerCluster * k + id.x] - maxResp);
    }
        
    float tmp = maxResp + log(sum);

    // Normalize responsibilities.
    for (k = 0; k != _NumClusters; ++k)
    {
        uint dstIndex = eltsPerCluster * k + id.x;
        float resp = exp(_RespsOut[dstIndex] - tmp) * binSize;

        // Responsibilities will be reduced in _Weights.
        _RespsOut[dstIndex] = resp;
        _WeightsOut[dstIndex] = resp;
        _MeansOut[dstIndex] = p * resp;
    }
}

groupshared float gs_Weights[GROUP_SIZE];
groupshared float3 gs_Means[GROUP_SIZE];

// Reduce Means. Each thread processes all clusters.
[numthreads(GROUP_SIZE, 1, 1)]
void ReduceWeightsAndMeans(uint3 threadId : SV_GroupThreadID, uint3 groupId : SV_GroupID)
{
    #define groupIndex groupId.x
    #define localThreadIndex threadId.x
    uint globalThreadIndex = groupIndex * GROUP_SIZE + localThreadIndex;

    uint srcCount = _IndirectArgsBufferIn[_IndirectArgsOffset + 3];
    uint dstCount = _IndirectArgsBufferIn[_IndirectArgsOffset + 0];
    uint srcOffset = srcCount * _ClusterIndex;
    uint dstOffset = dstCount * _ClusterIndex;

    // Copy to group memory.
    if (globalThreadIndex < srcCount)
    {
        gs_Weights[localThreadIndex] = _WeightsIn[srcOffset + globalThreadIndex];
        gs_Means[localThreadIndex] = _MeansIn[srcOffset + globalThreadIndex];
    }
    else
    {
        gs_Weights[localThreadIndex] = 0;
        gs_Means[localThreadIndex] = (0).xxx;
    }

    GroupMemoryBarrierWithGroupSync();

    // See https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf
    [unroll]
    for (uint s = GROUP_SIZE / 2u; s > 0u; s >>= 1u)
    {
        if (localThreadIndex < s)
        {
            gs_Weights[localThreadIndex] += gs_Weights[localThreadIndex + s];
            gs_Means[localThreadIndex] += gs_Means[localThreadIndex + s];
        }

        GroupMemoryBarrierWithGroupSync();
    }

    // Copy to global memory.
    if (localThreadIndex == 0u)
    {
        _WeightsOut[dstOffset + groupIndex] = gs_Weights[0];
        _MeansOut[dstOffset + groupIndex] = gs_Means[0];
    }
}

[numthreads(GROUP_SIZE, 1, 1)]
void NormalizeMeansAndFracs(uint3 id : SV_DispatchThreadID)
{
    if (id.x > _NumClusters - 1)
    {
        return;
    }

    float weight = _WeightsIn[id.x];
    _MeansOut[id.x] /= weight;
    _FracsOut[id.x] = weight / _TotalSamples;
}

[numthreads(GROUP_SIZE, 1, 1)]
void UpdateCovariances(uint3 id : SV_DispatchThreadID)
{
    uint index = _IndirectArgsBufferIn[3] * _ClusterIndex + id.x;
    uint2 indexAndSize = _SelectedColorBins[id.x];
    float3 p = GetVoxelCenter(indexAndSize.x);

    float resp = _RespsIn[index];
    float3 mean = _MeansIn[_ClusterIndex];

    float3x3 covariance = 0;

    // TODO Optimize?
    [unroll(3)]
    for(uint i = 0; i != 3; ++i)
    {
        [unroll(3)]
        for(uint j = 0; j != 3; ++j)
        {
            covariance[i][j] = (p[i] - mean[i]) * (p[j] - mean[j]);
        }
    }
    
    WriteMatrixSymmetric3x3(_CovariancesOut, index, covariance * resp);
}

groupshared float2x3 gs_Covariances[GROUP_SIZE];

[numthreads(GROUP_SIZE, 1, 1)]
void ReduceCovariances(uint3 threadId : SV_GroupThreadID, uint3 groupId : SV_GroupID)
{
    #define groupIndex groupId.x
    #define localThreadIndex threadId.x
    uint globalThreadIndex = groupIndex * GROUP_SIZE + localThreadIndex;

    uint srcCount = _IndirectArgsBufferIn[_IndirectArgsOffset + 3];
    uint dstCount = _IndirectArgsBufferIn[_IndirectArgsOffset + 0];
    uint srcOffset = srcCount * _ClusterIndex;
    uint dstOffset = dstCount * _ClusterIndex;

    GroupMemoryBarrierWithGroupSync();

    if (globalThreadIndex < srcCount)
    {
        gs_Covariances[localThreadIndex] = _CovariancesIn[srcOffset + globalThreadIndex];
    }
    else
    {
        gs_Covariances[localThreadIndex] = (float2x3)0;
    }

    GroupMemoryBarrierWithGroupSync();

    // See https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf
    [unroll]
    for (uint s = GROUP_SIZE / 2u; s > 0u; s >>= 1u)
    {
        if (localThreadIndex < s)
        {
            gs_Covariances[localThreadIndex] += gs_Covariances[localThreadIndex + s];
        }

        GroupMemoryBarrierWithGroupSync();
    }

    if (localThreadIndex == 0u)
    {
        _CovariancesOut[dstOffset + groupIndex] = gs_Covariances[0];
    }
}

[numthreads(GROUP_SIZE, 1, 1)]
void NormalizeCovariances(uint3 id : SV_DispatchThreadID)
{
    if (id.x > _NumClusters - 1)
    {
        return;
    }

    // Normalization.
    float3x3 covariance = ReadMatrixSymmetric3x3(_CovariancesOut, id.x) / _WeightsIn[id.x];

    // Prevent covariance from becoming singular. 
    float eps = 1e-6;
    covariance[0][0] += eps;
    covariance[1][1] += eps;
    covariance[2][2] += eps;
    
    WriteMatrixSymmetric3x3(_CovariancesOut, id.x, covariance);
}
